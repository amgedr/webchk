#!/usr/bin/env python3

"""
webchk v0.1 - Check HTTP status codes, response headers and redirects.
This is a free software and it comes with absolutely no warranty.
You can distribute and modify it under terms of BSD Revised License.
Homepage: http://codehill.com/projects/webchk

Usage:
  webchk [-alpsf] URL... [-o <file>]
  webchk [-alpsf] -i <file> [-o <file>]
  webchk -h, --help
  webchk -v, --version

Options:
  -i <file>, --input <file>     Read input from a file
  -o <file>, --output <file>    Save output to a file
  -p, --parse                   Parse contents of URLs with .xml extention
  -a, --all                     Display the complete HTTP header
  -l, --list                    Print URLs without checking them
  -s, --summary                 Print a summary only
  -f, --format                  Format the URLs heirarchically
  -h, --help                    Show this
  -v, --version                 Print the version number
"""

import os
import sys
import socket
from docopt import docopt
import http.client
from urllib.parse import urlparse
import xml.etree.ElementTree as etree
from datetime import datetime

arguments = docopt(__doc__, help=True, version="webchk 0.1")
url_count = 0
http_resp = {}


def main():
    try:
        urls_list = []

        # if an input file is provided, place its content in urls_list
        if arguments["--input"]:
            # check if the file exists.
            if os.path.isfile(arguments["--input"]):
                # read all non-empty lines for the input file
                urls_list = filter(
                    None, (u.rstrip() for u in open(arguments["--input"])))
            else:
                print("Error: Input file does not exists.", file=sys.stderr)

        # if any URLs were provided in the command-line
        if arguments["URL"]:
            urls_list.extend(arguments["URL"])

        if arguments["--output"]:
            sys.stdout = open(arguments["--output"], "w")

        for u in urls_list:
            http_get(u, 0)

        print_summary()
        sys.stdout.close()

    except (KeyboardInterrupt, SystemExit):
        print_summary()
        print("\r  \nPocess cancelled by user", file=sys.stderr)
        sys.exit(1)
    except PermissionError as err:
        f = getattr(err, 'filename', "")
        print("Error: could not read file {}. Permission denied.".format(f),
              file=sys.stderr)
        sys.exit(3)
    except Exception as err:
        print("Error: {}".format(sys.exc_info()), file=sys.stderr)
        sys.exit(2)


def http_get(url, level):
    global url_count, http_resp
    url_count += 1

    try:
        loc = urlparse(url)

        # if the scheme (http, https ...) is not available urlparse wont work
        if loc.netloc == "":
            url = "http://" + url
            loc = urlparse(url)

        print('\n', loc)
        start_time = datetime.now()

        conn = http.client.HTTPConnection(loc.netloc) 

        if loc.path.endswith(".xml") and arguments["--parse"]:
            conn.request("GET", loc.path)   # download the contents of sitemap
            resp = conn.getresponse()

            # calculate download time in microseconds
            time = (datetime.now() - start_time).total_seconds()

            print_url(url, resp, time, level)

            if resp.status != 200:
                return

            if arguments["--all"]:
                for i, v in resp.getheaders():
                    if arguments["--format"]:
                        print("{}".format(" " * (level + 1)), end="")

                    print("{}: {}".format(i, v))

            data = resp.read()  # read the contents of the XML file
            rootxml = etree.fromstring(data)

            for i in rootxml:
                for j in i:
                    if j.tag.endswith("loc"):  # get the <loc> tag's contents
                        http_get(j.text.strip(), level + 1)

        else:
            conn.request("HEAD", loc.path)
            resp = conn.getresponse()

            # calculate download time in microseconds
            time = (datetime.now() - start_time).total_seconds()

            if arguments["--list"]:
                if arguments["--format"]:
                    print("{}".format(" " * level), end="")

                print(loc.geturl())
                return

            print_url(url, resp, time, level)

            if arguments["--all"]:
                for i, v in resp.getheaders():
                    if arguments["--format"]:
                        print(" ")

                    print("{}: {}".format(i, v))

            # if response code is 301 Moved Permanently or 302 Redirected
            if resp.status == 301 or resp.status == 302:
                # get the URL being redirected to
                http_get(resp.getheader("Location"), level + 1)

        key = "{} {}".format(resp.status, resp.reason)

        # increment stats if exist, otherwise create it
        http_resp[key] = http_resp.get(key, 0) + 1

    except (KeyboardInterrupt, SystemExit):
        raise
    except socket.gaierror:
        print(url, "- Could not resolve name", file=sys.stderr)
    except BaseException as err:
        print("{} - Error: {}".format(url, err), file=sys.stderr)


def print_url(url, resp, time, level):
    if arguments["--format"]:
        print("{}".format(" " * level), end="")

    print("{} - {} {} ({:.3f}s)".format(url, resp.status, resp.reason, time))


def print_summary():
    if url_count == 0:
        return

    if arguments["--summary"]:
        print()
        for status, count in http_resp.items():
            print("{}: {}".format(status, count))

        print("\nTotal URLs: {}".format(url_count))


if __name__ == "__main__":
    main()
